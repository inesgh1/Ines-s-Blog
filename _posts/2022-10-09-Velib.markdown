---
layout: post
title: Analyse des donn√©es Velib "Etude de cas"
date: 2022-10-09
description: C'est un projet qui etait propos√© comme etant un test technique pour le poste stagiaire data scientist
img: velib.png # Add image post (optional)
tags: [Programming,K-means,machine learning,interview,data science] # add tag
---
Ce projet est a la base un test technique pour le poste stagiare data scientist.

**Theme:** Mobilit√© et Espace Public

**Sujet:** disponibilit√© des velos en temps reel

**Angle d'etude**: Comment on peut classer les quartiers selon l'activit√© du reseau Velib!!

## Plan
1. Recuperation des donn√©es de l'API velib
2. Stockage des donn√©es
3. Acquisition des donn√©es
4. Comprendre notre jeu de donn√©e
5. Classification automatique non supervis√©e

la 1 ere etape c'est de comprendre le jeu de donn√©e velib qui est du [opendata-paris](https://opendata.paris.fr/explore/dataset/velib-disponibilite-en-temps-reel/information/?disjunctive.name&disjunctive.is_installed&disjunctive.is_renting&disjunctive.is_returning&disjunctive.nom_arrondissement_communes)
mais avant on a besoin de recuperer les donn√©e avec python en utilisant API .


# 1.Recuperation des donn√©es de l'API velib

Plus concr√®tement, regardons en d√©tail ce que contiennent ces donn√©es avec Python. Pour ce faire, nous allons utiliser le module requests pour lancer une requ√™te sur l'API de V√©lib. Notre premi√®re requ√™te porte sur les caract√©ristiques des stations V√©libs
```
import requests

answer = requests.get("https://velib-metropole-opendata.smoove.pro/opendata/Velib_Metropole/station_information.json").json()
print(answer.keys())
```
On obtient en retour un fichier JSON, que l'on peut manipuler comme un dictionnaire Python, avec 3 clefs : lastUpdatedOther l'indication de la date de derni√®re mise √† jour des donn√©es, ttl la dur√©e de vie des donn√©es avant de devenir obsol√®te et enfin data les donn√©es proprement d√Ætes.

Pour manipuler ces donn√©es, nous allons utiliser le module Pandas. C'est un outil puissant d'analyse de donn√©es dont la base est l'objet DataFrame

```
import pandas as pd

df_1 = pd.DataFrame(answer["data"]["stations"])
df_1.head()
```
On voit que pour chaque station, nous disposons de son nom, de son identifiant unique station_id et de sa position en latitude lat et longitude lon. Ce sont des donn√©es √† dur√©e de vie longue que nous allons garder sous le bras. On ne s'attend pas √† ce que des stations se d√©placent ou bien changent de nom toutes les 5 minutes.

Une requ√™te que nous allons effectuer tr√®s r√©guli√®rement concerne l'occupation des stations :
```
answer = requests.get("https://velib-metropole-opendata.smoove.pro/opendata/Velib_Metropole/station_status.json").json()
df_2 = pd.DataFrame(answer["data"]["stations"])
df_2.head()
```
Pour chaque station, nous obtenons en temps r√©el (les donn√©es sont mises √† jour toutes les minutes) le nombre de v√©libs mis √† disposition, le nombre de places disponibles et m√™me la d√©composition entre le nombre de v√©los m√©caniques et √©lectriques.
  Congrats üéâ:Mission accomplie nous avons recuperer les donn√©e en utilisant API et python.
 
# 2.Stockage des donn√©es

Maintenant que nous savons r√©cup√©rer les donn√©es V√©lib, nous allons pouvoir les stocker sous la forme d'une base de donn√©es persistante SQLITE3. Pour faire simple, il s'agit d'une base de donn√©es dite l√©g√®re (mais puissante !) qui se pr√©sente sous la forme d'un simple fichier. C'est un format tr√®s pratique pour de petits projets comme le notre ou pour la phase de d√©veloppement de plus gros projets. Avec Pandas, la proc√©dure est limpide :
```
# On supprimse les donn√©es inutiles
del df_1["stationCode"]
del df_1["rental_methods"]
del df_2["numBikesAvailable"]
del df_2["num_bikes_available_types"]
del df_2["numDocksAvailable"]
del df_2["is_installed"]
del df_2["is_returning"]
del df_2["is_renting"]
del df_2["last_reported"]

# On cr√©e un marqueur temporel
time_stamp = pd.Timestamp.now()
df_2["time_stamp"] = time_stamp

# On enregistre sous forme de base SQLITE
df_1.to_sql("localisation", "sqlite:///data.db", if_exists="replace")
df_2.to_sql("stations", "sqlite:///data.db", if_exists="append")

```
La commande se lit : j'envoie le DataFrame df_1 vers SQL sous le nom de table localisation et dans la base SQLITE data.db. Si la table existe alors on la remplace. Pour les donn√©es en temps r√©el, on ajoute un marqueur temporel qui permet de garder en m√©moire l'heure d'acquisition des donn√©es. Notez l'option if_exists="append" : √† chaque nouvelle acquisition, on ajoute de nouvelles lignes √† la table. Concr√®tement, le marqueur temporel ressemble √† √ßa
```
time_stamp
```

```
output:Timestamp('2022-10-09 01:38:18.399282')
```
Le marqueur retient la date et l'heure pr√©cise √† laquelle j'√©cris ces lignes. Et ce sera la m√™me chose pour nos donn√©es d'occupation des stations V√©lib.

# 3. Acquisition des donn√©es
Il ne nous reste plus qu'√† automatiser l'acquisition de donn√©es toutes les minutes. Il existe un module python pour √ßa qui s'appelle APScheduler. Ce module permet l'automatisation et la planification de t√¢ches, et nous utiliserons l'objet BlockingScheduler. Son utilisation est la suivante :
```
from apscheduler.schedulers.blocking import BlockingScheduler

sched = BlockingScheduler()

@sched.scheduled_job("interval", seconds=5)
def print_date():
    time_stamp = pd.Timestamp.now()
    print(time_stamp)

sched.start()
```
Avec le d√©corateur @sched.scheduled_job("interval", seconds=5), on indique que la fonction doit √™tre appel√©e r√©guli√®rement avec un intervalle de temps de 5 secondes.

Nous avons maintenant tous les ingr√©dients pour lancer une acquisition de donn√©es. 
# 4. Comprendre notre jeu de donn√©e
Nous allons maintenant pouvoir jouer avec nos belles donn√©es toutes fraiches. Commen√ßons par importer les deux tables dans deux DataFrames distincts
```
data_stations = pd.read_sql_table("stations", "sqlite:///database.db")
data_localisation = pd.read_sql_table("localisation", "sqlite:///database.db")

data_stations.head()
```
Notre table stations contient environ 2 millions de lignes, un index et 5 colonnes : le code de la station stationCode, l'identifiant de la station station_id, le nombre de v√©los disponibles num_bikes_available, le nombre de bornes libres num_docks_available ainsi que notre marqueur temporel time__stamp.

### Manipulation des donn√©es : jointure
Pour en savoir plus sur une station, il faut faire la correspondance avec notre table localisation. Une premi√®re approche serait de faire une s√©lection sur cette autre table avec notre station_id, et cela fonctionne tr√®s bien !
```
data_localisation[data_localisation["station_id"] == 213688169]
```
![I and My friends]({{site.baseurl}}/assets/img/station.png)

Cette myst√©rieuse station se nomme donc Benjamin Godard - Victor Hugo avec une capacit√© de 35 v√©los et nous avons m√™me ses coordonn√©es g√©ographiques ! On pourrait de la m√™me mani√®re mettre en relation chacune de nos observations avec des s√©lections, mais ce serait tr√®s inefficace. La bonne mani√®re de proc√©der, bien connue quand on manipule des bases de donn√©es, c'est de faire une jointure.

Dit simplement, on va fusionner les deux tables et aligner les lignes qui partagent certaines propri√©t√©s. Ici, nous souhaitons mettre en commun les informations sur les stations qui partagent un m√™me identifiant station_id. Il existe plusieurs types de jointures et nous ne nous attarderons pas davantage sur la d√©nomination exacte. Avec Pandas, la syntaxe est plut√¥t claire
```
data_stations = data_stations.merge(data_localisation, on="station_id")

data_stations
```
:confetti_ball: √Ä l'issue de cette fusion, nous disposons d'une table plus grande, avec le m√™me nombre de lignes (et donc d'observations), mais disposant de cinq colonnes suppl√©mentaires issues de la seconde table. Nous avons ici, toutes les donn√©es n√©cessaires pour commencer l'analyse spatiale de ces fameuses donn√©es V√©lib.
### Faisons quelques statistiques de base¬†:
```
print("There are {0} Velib stands in Paris".format(velib_data.address.count()))
print("There are {0} bike stands in total".format(velib_data.bike_stands.sum()))
print("There are {0} available bikes".format(velib_data.available_bikes.sum()))
print("There are {0} available bikes stands".format(velib_data.available_bike_stands.sum()))
print("")

bike_stands_max = velib_data.bike_stands.max()
bike_stands_max_query = "bike_stands == " + str(bike_stands_max)
print("Biggest stations with {0} bike stands:".format(bike_stands_max))
print(velib_data.query(bike_stands_max_query).address.values)
print("")

bike_stands_min = velib_data.bike_stands.min()
bike_stands_min_query = "bike_stands == " + str(bike_stands_min)
print("Smallest stations with {0} bike stands:".format(bike_stands_min))
print(velib_data.query(bike_stands_min_query).address.values)
```
![success]({{site.baseurl}}/assets/img/stat1.png)
**Nombre de stands par station:**
```
stands.hist();
title("Number of bike stands per station.");
```
![success]({{site.baseurl}}/assets/img/sb.png)
Maintenant, on recupere un code postal des adresses en utilisant une expression r√©guli√®re, et on le stocke dans une nouvelle colonne. On pressente ensuite un **histogramme pour montrer le nombre de stations par zone:**
```
import re
velib_data['postcode'] = velib_data['address'].apply(lambda x: re.findall('\d{5}',x)[0] )
plt.figure(figsize=(10, 6))
velib_data.groupby('postcode').size().plot.bar();
plt.tight_layout()
```
![success]({{site.baseurl}}/assets/img/histo.png)
### Analyse spatiale

nous allons analyser ces donn√©es spatiales en utilisant la librairie GeoPandas. Cette librairie pr√©sente beaucoup de similarit√© avec Pandas. L'objet de base s'appelle un GeoDataFrame, il s'agit d'un DataFrame avec une colonne sp√©ciale nomm√©e geometry qui contient des renseignements g√©ographiques. Cette geometry peut √™tre un point, comme dans le cas pr√©sent, mais aussi une ligne pour d√©signer une fronti√®re ou bien un polygone pour mat√©rialiser un territoire. Nous reviendrons sur ce dernier point dans quelques instants.

La premi√®re √©tape consiste √† convertir notre couple (longitude, latitude) en un object Point reconnu par GeoPandas, puis √† cr√©er notre GeoDataFrame.
```
import geopandas as gpd

# Convert the longitude and latitude to a format recognized by geoPandas
geometry = gpd.points_from_xy(data_stations["lon"], data_stations["lat"])

# Create a DataFrame with a geometry containing the Points
geo_stations = gpd.GeoDataFrame(
    data_stations, crs="EPSG:4326", geometry=geometry
)

geo_stations
```
L'acronyme crssignifie Coordinate Reference System, c'est une indication du syst√®me de projection utilis√©. En regardant la documentation V√©lib, on voit que le r√©f√©rentiel de projection utilis√© est WGS84. C'est le syst√®me de projection le plus commun aujourd'hui et il est notamment utilis√© par les syst√®mes de positionnement par satellite GPS. Ce syst√®me est r√©f√©renc√© 4326 en deux dimensions (X,Y) et 4979 en trois dimensions (X,Y,Z) selon la liste des codes EPSG, et c'est ce que nous donnons comme indication √† notre GeoDataFrame.
### premi√®res cartes
Avant de faire une carte, nous allons s√©lectionner des donn√©es √† un temps fix√©, et les enregistrer dans un nouveau GeoDataFrame
```
some_time = geo_stations["time_stamp"][0]
some_data = geo_stations[geo_stations["time_stamp"] == some_time]

some_time
```
Maintenant nous allons pouvoir ajouter un fond de carte provenant, par exemple, de OpenStreetMap. Il existe un module python pour faire √ßa en une ligne et cette librairie magique s'appelle contextily. Le seul pr√©-requis est de convertir les coordonn√©es GPS au format EPSG:3857, mais GeoPandas fait √ßa tr√®s bien
```
import contextily as ctx

# Conversion des coordonn√©es
some_data = some_data.to_crs(epsg=3857)

fig, ax = plt.subplots(figsize=(8, 6))

ax.set_title(some_time.strftime("%A %B %d %H:%M"))
some_data.plot("num_bikes_available", markersize="num_bikes_available", cmap="OrRd", ax=ax)
ax.set_axis_off()

# Ajout du fond de carte
ctx.add_basemap(ax)

plt.show()
```
![success]({{site.baseurl}}/assets/img/plot.png)

### Aller plus loin : rassembler les donn√©es par quartiers
L'inconv√©nient majeur de nos cartes est qu'il est difficile de distinguer et de hi√©rarchiser les zones √† forte concentration de v√©libs. La raison en est que les marqueurs ont tendance √† se chevaucher et que l'on perd en lisibilit√©. Pour rem√©dier √† ce probl√®me, nous allons rassembler les stations appartenant √† un m√™me quartier. Ainsi, nous allons tracer des surfaces sur la carte au lieu d'un nuage de points. La liste des quartiers administratifs de Paris est mise √† disposition sur [ le site de la ville de Paris](https://opendata.paris.fr/explore/dataset/quartier_paris/information/?disjunctive.c_ar). GeoPandas sait parfaitement importer les fichiers GeoJSON
```
districts = gpd.read_file("quartier_paris.geojson").to_crs(epsg=3857)
districts
```
Chaque quartier est donc d√©fini comme un polygone, avec un certain p√©rim√®tre et une surface. On peut directement tracer les quartiers de Paris sur la carte.
```
fig, ax = plt.subplots(figsize=(8, 6))

districts.plot(ax=ax, alpha=0.5, edgecolor="white")
ax.set_axis_off()
ctx.add_basemap(ax)

plt.show()
```
![success]({{site.baseurl}}/assets/img/quartier.png)

On aimerait disposer du nombre de v√©libs pr√©sent dans chacun de ces quartiers, et c'est l√† que se d√©ploie toute la puissance de GeoPandas, voyez plut√¥t
```
districts["velib_number"] = districts.apply(
    lambda district: (
        some_data.within(district.geometry) * some_data["num_bikes_available"]
    ).sum(),
    axis=1
)
```
Ce bloc m√©rite quelques explications. On cr√©e une nouvelle colonne nomm√©e velib_number, c'est √† dire le nombre de v√©libs pr√©sents dans le quartier. Pour ce faire, on somme tous les v√©los qui sont inclus dans le quartier en question. Cette op√©ration est rendue possible par la m√©thode .within() qui va d√©terminer si la station est bien localis√©e dans le polygone repr√©sentant le quartier.

On peut faire la m√™me op√©ration pour calculer le nombre maximum de v√©libs dans le quartier, et donc le pourcentage d'occupation des stations dans le quartier
```
districts["max_velib_number"] = districts.apply(
        lambda district: (
            some_data.within(district.geometry) * some_data["capacity"]
        ).sum(),
        axis=1,
)

districts["occupation"] = districts["velib_number"] / districts["max_velib_number"]
```
Enfin, on va tracer le taux d'occupation des stations V√©lib, quartier par quartier
```
fig, ax = plt.subplots(figsize=(8, 6))

districts.plot("occupation", cmap="OrRd", ax=ax, alpha=0.5, edgecolor="white")
ax.set_axis_off()
ctx.add_basemap(ax)

plt.show()
```
![success]({{site.baseurl}}/assets/img/occupation.png)

On peut distinguer √† l'oeil quelques tendances suivant les quartiers, certains sont actifs la nuit et d'autres le jour. Mais est-il possible de faire mieux qu'une simple analyse √† l'oeil ? 

 Je vous propose une approche compl√®tement automatique pour classifier les diff√©rents types de quartier dans Paris, en utilisant l'algorithme des -moyennes.
### S√©ries temporelles par quartier
Le r√©seau V√©lib compte environ 1400 stations et il nous serait difficile de les √©tudier toutes en m√™me temps. Nous commen√ßons donc par regrouper les donn√©es par quartier et par heure.
```
# Jointure spatiale : on identifie le quartier de chaque observation
geo_districts = gpd.sjoin(districts, geo_stations, how="left")

# On aggr√®ge les donn√©es par quartier et par heure
occupation_data = geo_districts.groupby(["l_qu", "time_stamp"], as_index=False).agg({"num_bikes_available":"sum", "geometry": "first"})

occupation_data.head()
```
La syntaxe est ici plus compacte que dans le pr√©c√©dent √©pisode donc nous allons la d√©cortiquer ensemble. Dans un premier, nous groupons les donn√©es en fonction de l_qu, le nom du quartier et de time_stamp, c'est-√†-dire l'heure d'observation. √Ä l'issue de cette op√©ration, on obtient un objet DataFrameGroupBy. Notez l'option as_index=False qui est n√©cessaire ici.

Puis nous aggr√©geons les donn√©es avec la m√©thode .agg(). En argument, on lui dit qu'on veut sommer le nombre de v√©los disponibles dans chaque groupe, et garder la g√©om√©trie associ√©e au quartier. On peut ne garder que la premi√®re occurence, puisque toutes ces g√©om√©tries sont identiques.

Nous avons donc l'historique, minute par minute, du nombre de v√©libs stationn√©s dans chaque quartier, que nous pouvons visualiser de la fa√ßon suivante:
```
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

fig, ax = plt.subplots(figsize=(8,6))

occupation_data.groupby('l_qu').plot(x="time_stamp", y="num_bikes_available", kind="line", ax=ax, legend=False)
ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))

plt.show()
```
![success]({{site.baseurl}}/assets/img/tempo.png)


Chacune de ces courbes est une ***s√©rie temporelle*** qui caract√©rise l'activit√© du r√©seau V√©lib dans son quartier. Ce diagramme est un peu brouillon, mais on peut distinguer √† l'oeil quelques tendances. Certaines stations semblent se vider en journ√©e quand d'autres semblent se remplir dans le m√™me interval. Mais peut-on faire mieux et de mani√®re automatique ?

### Algorithme des k-moyennes
La question que l'on se pose est la suivante : "Est-il possible de classer ces s√©ries temporelles de mani√®res automatique ?". La r√©ponse est oui, et pour cela nous allons utiliser l'algorithme des -moyennes. Cet algorithme permet de diviser des donn√©es en  partitions. Sa particularit√© est de pouvoir √™tre mis en place sans supervisition. C'est √† dire qu'il va apprendre tout seul √† faire la distinction entre les diff√©rentes partitions sans que nous ayons besoin d'intervenir, et c'est pr√©cis√©ment ce qu'on lui demande.

### Applications aux donn√©es temporelles
Commen√ßons l'analyse en mettant nos donn√©es sous la forme de s√©ries temporelles pour la librairie tslearn
```
from tslearn.utils import to_time_series_dataset
from tslearn.preprocessing import TimeSeriesScalerMeanVariance

labels, time_series = [] , []
for l_qu, group in occupation_data.groupby("l_qu"):
    labels.append(l_qu)
    time_series.append(group["num_bikes_available"].array)

# On formatte les s√©ries temporelles
time_series = to_time_series_dataset(time_series)
# On normalise ces m√™mes s√©ries
time_series = TimeSeriesScalerMeanVariance().fit_transform(time_series)

time_series.shape
```
Rien de particulier ici, si ce n'est que la fonction to_time_series_dataset cr√©e un jeu de donn√©es pour tslearn √† partir d'un tableau, et que nous normalisons ces donn√©es avec l'aide de TimeSeriesScalerMeanVariance qui fait en sorte que chaque s√©rie ait pour moyenne  et variance . Cette √©tape est tr√®s importante puisqu'elle permet de s'affranchir de nombreux biais comme le nombre absolu de v√©libs par quartier, ou la proportion d'habitants utilisant le r√©seau.

On peut maintenant lancer l'apprentissage non-supervis√© de notre mod√®le avec l'algorithme des -moyennes

```
from tslearn.clustering import TimeSeriesKMeans
n_classes = 3

model = TimeSeriesKMeans(n_clusters=n_classes, metric="euclidean")
model.fit(time_series)
TimeSeriesKMeans()
```
Visualisons les barycentres (en rouge) et les diff√©rents √©l√©ments (gris) de chaque classe que notre mod√®le a trouv√©
```
import matplotlib.dates as mdates

fig = plt.figure(figsize=(8,n_classes * 3))

# On r√©cup√®re les dates des diff√©rentes observations
time_labels = occupation_data["time_stamp"].unique()

# Pour chaque classe
for yi in range(n_classes):
    ax = fig.add_subplot(n_classes, 1, 1 + yi)

    # On s√©lectionne les s√©ries qui correspondent √† cette classe
    for xx in time_series[model.labels_ == yi]:
        ax.plot(time_labels, xx, "k-", alpha=.2)

    # Le barycentre de la classe
    ax.plot(time_labels, model.cluster_centers_[yi].ravel(), "r-")

    # Pour formatter l'heure des observations
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))

plt.show()
```
![success]({{site.baseurl}}/assets/img/kmeans.png)

### Interpr√©tation et visualisation
Notre algortihme a class√© automatiquement tous les quartiers en trois cat√©gories bien distinctes que l'on peut interpr√©ter comme √©tant :

#### 1. Les quartiers d'affaires qui se remplissent √† 9h du matin et se vident aux alentours de 18h
#### 2. Les quatiers r√©sidentiels qui se remplissent durant la nuit et se vident √† 9h
#### 3. Les quartiers d'activit√©s nocturnes qui sont actifs de 18h √† minuit environ
Maintenant que nous avons d√©termin√© la classe de chaque quartier, nous pouvons l'ajouter √† notre liste des quartiers de Paris
```
interpretation = ["Affaire", "R√©sidentiel", "Nocturne"]

classification = pd.DataFrame({"l_qu":labels, "type": [interpretation[classe] for classe in model.labels_] })
districts_classified = districts.merge(classification, on="l_qu")

districts_classified.head()
```
![success]({{site.baseurl}}/assets/img/classement.png)
Et produire la carte des types de quartiers dans Paris √† partir des donn√©es V√©libs
```
import contextily as ctx

fig, ax = plt.subplots(figsize=(8, 6))

districts_classified = districts_classified.to_crs(epsg=3857)

districts_classified.plot("type", ax=ax, alpha=0.5, edgecolor="white", legend=True)
ax.set_axis_off()
ctx.add_basemap(ax)

plt.show()
```
![success]({{site.baseurl}}/assets/img/map.png)




































