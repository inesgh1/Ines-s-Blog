---
layout: post
title: Analyse des donn√©es Velib "Etude de cas"
date: 2022-10-09
description: C'est un projet qui etait propos√© comme etant un test technique pour le poste stagiaire data scientist
img: velib.png # Add image post (optional)
tags: [Programming,K-means,machine learning,interview,data science] # add tag
---
Ce projet est a la base un test technique pour le poste stagiare data scientist.
la 1 ere etape c'est de comprendre le jeu de donn√©e velib qui est du [opendata-paris](https://opendata.paris.fr/explore/dataset/velib-disponibilite-en-temps-reel/information/?disjunctive.name&disjunctive.is_installed&disjunctive.is_renting&disjunctive.is_returning&disjunctive.nom_arrondissement_communes)
apres on a...

## Plan
1. Acquisition des donn√©es V√©lib avec Python et Pandas
2. Stockage des donn√©es
3. Acquisition des donn√©es
4. Comprendre notre jeu de donn√©e
5. Classification automatique non supervis√©e
6. Data visualization

# 1.Acquisition des donn√©es V√©lib avec Python et Pandas

Plus concr√®tement, regardons en d√©tail ce que contiennent ces donn√©es avec Python. Pour ce faire, nous allons utiliser le module requests pour lancer une requ√™te sur l'API de V√©lib. Notre premi√®re requ√™te porte sur les caract√©ristiques des stations V√©libs
```
import requests

answer = requests.get("https://velib-metropole-opendata.smoove.pro/opendata/Velib_Metropole/station_information.json").json()
print(answer.keys())
```
On obtient en retour un fichier JSON, que l'on peut manipuler comme un dictionnaire Python, avec 3 clefs : lastUpdatedOther l'indication de la date de derni√®re mise √† jour des donn√©es, ttl la dur√©e de vie des donn√©es avant de devenir obsol√®te et enfin data les donn√©es proprement d√Ætes.

Pour manipuler ces donn√©es, nous allons utiliser le module Pandas. C'est un outil puissant d'analyse de donn√©es dont la base est l'objet DataFrame

```
import pandas as pd

df_1 = pd.DataFrame(answer["data"]["stations"])
df_1.head()
```
On voit que pour chaque station, nous disposons de son nom, de son identifiant unique station_id et de sa position en latitude lat et longitude lon. Ce sont des donn√©es √† dur√©e de vie longue que nous allons garder sous le bras. On ne s'attend pas √† ce que des stations se d√©placent ou bien changent de nom toutes les 5 minutes.

Une requ√™te que nous allons effectuer tr√®s r√©guli√®rement concerne l'occupation des stations :
```
answer = requests.get("https://velib-metropole-opendata.smoove.pro/opendata/Velib_Metropole/station_status.json").json()
df_2 = pd.DataFrame(answer["data"]["stations"])
df_2.head()
```
Pour chaque station, nous obtenons en temps r√©el (les donn√©es sont mises √† jour toutes les minutes) le nombre de v√©libs mis √† disposition, le nombre de places disponibles et m√™me la d√©composition entre le nombre de v√©los m√©caniques et √©lectriques.
  Congrats üéâ:Mission accomplie nous avons recuperer les donn√©e en utilisant API et python.
  
  # 2.Stockage des donn√©es

Maintenant que nous savons r√©cup√©rer les donn√©es V√©lib, nous allons pouvoir les stocker sous la forme d'une base de donn√©es persistante SQLITE3. Pour faire simple, il s'agit d'une base de donn√©es dite l√©g√®re (mais puissante !) qui se pr√©sente sous la forme d'un simple fichier. C'est un format tr√®s pratique pour de petits projets comme le notre ou pour la phase de d√©veloppement de plus gros projets. Avec Pandas, la proc√©dure est limpide :
```
# On supprimse les donn√©es inutiles
del df_1["stationCode"]
del df_1["rental_methods"]
del df_2["numBikesAvailable"]
del df_2["num_bikes_available_types"]
del df_2["numDocksAvailable"]
del df_2["is_installed"]
del df_2["is_returning"]
del df_2["is_renting"]
del df_2["last_reported"]

# On cr√©e un marqueur temporel
time_stamp = pd.Timestamp.now()
df_2["time_stamp"] = time_stamp

# On enregistre sous forme de base SQLITE
df_1.to_sql("localisation", "sqlite:///data.db", if_exists="replace")
df_2.to_sql("stations", "sqlite:///data.db", if_exists="append")

```
La commande se lit : j'envoie le DataFrame df_1 vers SQL sous le nom de table localisation et dans la base SQLITE data.db. Si la table existe alors on la remplace. Pour les donn√©es en temps r√©el, on ajoute un marqueur temporel qui permet de garder en m√©moire l'heure d'acquisition des donn√©es. Notez l'option if_exists="append" : √† chaque nouvelle acquisition, on ajoute de nouvelles lignes √† la table. Concr√®tement, le marqueur temporel ressemble √† √ßa
```
time_stamp
```

```
output:Timestamp('2022-10-09 01:38:18.399282')
```
Le marqueur retient la date et l'heure pr√©cise √† laquelle j'√©cris ces lignes. Et ce sera la m√™me chose pour nos donn√©es d'occupation des stations V√©lib.

# 3. Acquisition des donn√©es
Il ne nous reste plus qu'√† automatiser l'acquisition de donn√©es toutes les minutes. Il existe un module python pour √ßa qui s'appelle APScheduler. Ce module permet l'automatisation et la planification de t√¢ches, et nous utiliserons l'objet BlockingScheduler. Son utilisation est la suivante :
```
from apscheduler.schedulers.blocking import BlockingScheduler

sched = BlockingScheduler()

@sched.scheduled_job("interval", seconds=5)
def print_date():
    time_stamp = pd.Timestamp.now()
    print(time_stamp)

sched.start()
```
Avec le d√©corateur @sched.scheduled_job("interval", seconds=5), on indique que la fonction doit √™tre appel√©e r√©guli√®rement avec un intervalle de temps de 5 secondes.

Nous avons maintenant tous les ingr√©dients pour lancer une acquisition de donn√©es. 
# 4. Comprendre notre jeu de donn√©e
Nous allons maintenant pouvoir jouer avec nos belles donn√©es toutes fraiches. Commen√ßons par importer les deux tables dans deux DataFrames distincts
```
data_stations = pd.read_sql_table("stations", "sqlite:///database.db")
data_localisation = pd.read_sql_table("localisation", "sqlite:///database.db")

data_stations.head()
```
Notre table stations contient environ 2 millions de lignes, un index et 5 colonnes : le code de la station stationCode, l'identifiant de la station station_id, le nombre de v√©los disponibles num_bikes_available, le nombre de bornes libres num_docks_available ainsi que notre marqueur temporel time__stamp.

### Manipulation des donn√©es : jointure
Pour en savoir plus sur une station, il faut faire la correspondance avec notre table localisation. Une premi√®re approche serait de faire une s√©lection sur cette autre table avec notre station_id, et cela fonctionne tr√®s bien !
```
data_localisation[data_localisation["station_id"] == 213688169]
```
![I and My friends]({{site.baseurl}}/assets/img/station.png)

Cette myst√©rieuse station se nomme donc Benjamin Godard - Victor Hugo avec une capacit√© de 35 v√©los et nous avons m√™me ses coordonn√©es g√©ographiques ! On pourrait de la m√™me mani√®re mettre en relation chacune de nos observations avec des s√©lections, mais ce serait tr√®s inefficace. La bonne mani√®re de proc√©der, bien connue quand on manipule des bases de donn√©es, c'est de faire une jointure.

Dit simplement, on va fusionner les deux tables et aligner les lignes qui partagent certaines propri√©t√©s. Ici, nous souhaitons mettre en commun les informations sur les stations qui partagent un m√™me identifiant station_id. Il existe plusieurs types de jointures et nous ne nous attarderons pas davantage sur la d√©nomination exacte. Avec Pandas, la syntaxe est plut√¥t claire
```
data_stations = data_stations.merge(data_localisation, on="station_id")

data_stations
```
:confetti_ball: √Ä l'issue de cette fusion, nous disposons d'une table plus grande, avec le m√™me nombre de lignes (et donc d'observations), mais disposant de cinq colonnes suppl√©mentaires issues de la seconde table. Nous avons ici, toutes les donn√©es n√©cessaires pour commencer l'analyse spatiale de ces fameuses donn√©es V√©lib.

### Analyse spatiale

nous allons analyser ces donn√©es spatiales en utilisant la librairie GeoPandas. Cette librairie pr√©sente beaucoup de similarit√© avec Pandas. L'objet de base s'appelle un GeoDataFrame, il s'agit d'un DataFrame avec une colonne sp√©ciale nomm√©e geometry qui contient des renseignements g√©ographiques. Cette geometry peut √™tre un point, comme dans le cas pr√©sent, mais aussi une ligne pour d√©signer une fronti√®re ou bien un polygone pour mat√©rialiser un territoire. Nous reviendrons sur ce dernier point dans quelques instants.

La premi√®re √©tape consiste √† convertir notre couple (longitude, latitude) en un object Point reconnu par GeoPandas, puis √† cr√©er notre GeoDataFrame.
```
import geopandas as gpd

# Convert the longitude and latitude to a format recognized by geoPandas
geometry = gpd.points_from_xy(data_stations["lon"], data_stations["lat"])

# Create a DataFrame with a geometry containing the Points
geo_stations = gpd.GeoDataFrame(
    data_stations, crs="EPSG:4326", geometry=geometry
)

geo_stations
```
L'acronyme crssignifie Coordinate Reference System, c'est une indication du syst√®me de projection utilis√©. En regardant la documentation V√©lib, on voit que le r√©f√©rentiel de projection utilis√© est WGS84. C'est le syst√®me de projection le plus commun aujourd'hui et il est notamment utilis√© par les syst√®mes de positionnement par satellite GPS. Ce syst√®me est r√©f√©renc√© 4326 en deux dimensions (X,Y) et 4979 en trois dimensions (X,Y,Z) selon la liste des codes EPSG, et c'est ce que nous donnons comme indication √† notre GeoDataFrame.
### premi√®res cartes
Avant de faire une carte, nous allons s√©lectionner des donn√©es √† un temps fix√©, et les enregistrer dans un nouveau GeoDataFrame
```
some_time = geo_stations["time_stamp"][0]
some_data = geo_stations[geo_stations["time_stamp"] == some_time]

some_time
```
Maintenant nous allons pouvoir ajouter un fond de carte provenant, par exemple, de OpenStreetMap. Il existe un module python pour faire √ßa en une ligne et cette librairie magique s'appelle contextily. Le seul pr√©-requis est de convertir les coordonn√©es GPS au format EPSG:3857, mais GeoPandas fait √ßa tr√®s bien
```
import contextily as ctx

# Conversion des coordonn√©es
some_data = some_data.to_crs(epsg=3857)

fig, ax = plt.subplots(figsize=(8, 6))

ax.set_title(some_time.strftime("%A %B %d %H:%M"))
some_data.plot("num_bikes_available", markersize="num_bikes_available", cmap="OrRd", ax=ax)
ax.set_axis_off()

# Ajout du fond de carte
ctx.add_basemap(ax)

plt.show()
```
![success]({{site.baseurl}}/assets/img/plot.png)

### Aller plus loin : rassembler les donn√©es par quartiers
L'inconv√©nient majeur de nos cartes est qu'il est difficile de distinguer et de hi√©rarchiser les zones √† forte concentration de v√©libs. La raison en est que les marqueurs ont tendance √† se chevaucher et que l'on perd en lisibilit√©. Pour rem√©dier √† ce probl√®me, nous allons rassembler les stations appartenant √† un m√™me quartier. Ainsi, nous allons tracer des surfaces sur la carte au lieu d'un nuage de points. La liste des quartiers administratifs de Paris est mise √† disposition sur [ le site de la ville de Paris](https://opendata.paris.fr/explore/dataset/quartier_paris/information/?disjunctive.c_ar). GeoPandas sait parfaitement importer les fichiers GeoJSON
```
districts = gpd.read_file("quartier_paris.geojson").to_crs(epsg=3857)
districts
```
Chaque quartier est donc d√©fini comme un polygone, avec un certain p√©rim√®tre et une surface. On peut directement tracer les quartiers de Paris sur la carte.
```
fig, ax = plt.subplots(figsize=(8, 6))

districts.plot(ax=ax, alpha=0.5, edgecolor="white")
ax.set_axis_off()
ctx.add_basemap(ax)

plt.show()
```
![success]({{site.baseurl}}/assets/img/quartier.png)

On aimerait disposer du nombre de v√©libs pr√©sent dans chacun de ces quartiers, et c'est l√† que se d√©ploie toute la puissance de GeoPandas, voyez plut√¥t
```
districts["velib_number"] = districts.apply(
    lambda district: (
        some_data.within(district.geometry) * some_data["num_bikes_available"]
    ).sum(),
    axis=1
)
```
Ce bloc m√©rite quelques explications. On cr√©e une nouvelle colonne nomm√©e velib_number, c'est √† dire le nombre de v√©libs pr√©sents dans le quartier. Pour ce faire, on somme tous les v√©los qui sont inclus dans le quartier en question. Cette op√©ration est rendue possible par la m√©thode .within() qui va d√©terminer si la station est bien localis√©e dans le polygone repr√©sentant le quartier.

On peut faire la m√™me op√©ration pour calculer le nombre maximum de v√©libs dans le quartier, et donc le pourcentage d'occupation des stations dans le quartier
```
districts["max_velib_number"] = districts.apply(
        lambda district: (
            some_data.within(district.geometry) * some_data["capacity"]
        ).sum(),
        axis=1,
)

districts["occupation"] = districts["velib_number"] / districts["max_velib_number"]
```
Enfin, on va tracer le taux d'occupation des stations V√©lib, quartier par quartier
```
fig, ax = plt.subplots(figsize=(8, 6))

districts.plot("occupation", cmap="OrRd", ax=ax, alpha=0.5, edgecolor="white")
ax.set_axis_off()
ctx.add_basemap(ax)

plt.show()
```
![success]({{site.baseurl}}/assets/img/occupation.png)

























































